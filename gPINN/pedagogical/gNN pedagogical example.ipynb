{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class gNN:\n",
    "    def __init__(self, x, u, u_deri, layers, activation, lr, weight_g):\n",
    "        self.x = x\n",
    "        self.u = u\n",
    "        self.u_deri = u_deri\n",
    "        self.lr = lr\n",
    "        self.activation = activation\n",
    "        self.weight_g = weight_g\n",
    "        self.loss_log = []\n",
    "        self.loss_res_log = []\n",
    "        self.loss_deriv_log = []\n",
    "\n",
    "        self.weights, self.biases = self.initilize_NN(layers)\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "        self.x_tf = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n",
    "        self.u_tf = tf.placeholder(tf.float32, shape=[None, self.u.shape[1]])\n",
    "        self.u_deriv_tf = tf.placeholder(tf.float32, shape=[None, self.u_deri.shape[1]])\n",
    "\n",
    "        self.u_pred = self.net_u(self.x_tf)\n",
    "        self.u_deri_pred = self.u_deriv(self.u_pred, self.x_tf)\n",
    "\n",
    "        self.loss_res   = tf.reduce_mean(tf.square(self.u_tf - self.u_pred))\n",
    "        self.loss_deriv = tf.reduce_mean(tf.square(self.u_deriv_tf - self.u_deri_pred))\n",
    "        self.loss       = self.loss_res + self.weight_g*self.loss_deriv\n",
    "\n",
    "        steps_per_decay = 1000\n",
    "        decay_factor = 0.9\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.lr = tf.train.exponential_decay(learning_rate = lr,\n",
    "                                           global_step = self.global_step,\n",
    "                                           decay_steps = steps_per_decay,\n",
    "                                           decay_rate = decay_factor,\n",
    "                                           staircase = True\n",
    "                                           )\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss,\n",
    "                                            global_step=self.global_step)\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    def initilize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases  = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "\n",
    "    def neural_net(self, X, weights, biases, activation):\n",
    "        num_layers = len(weights) + 1\n",
    "        H = X\n",
    "        for l in range(0, num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = activation(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "\n",
    "    def net_u(self, x):\n",
    "        u = self.neural_net(x, self.weights, self.biases, self.activation)\n",
    "        return u\n",
    "\n",
    "    def u_deriv(self, u, x):\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        return u_x\n",
    "\n",
    "    def callback(self, loss, res, deriv):\n",
    "        print('Loss:%f,res:%f,deriv:%f'%(loss, res, deriv))\n",
    "\n",
    "    def train(self, max_iter=40000):\n",
    "        loss_value = np.inf\n",
    "        for iter in range(max_iter):\n",
    "            self.sess.run(self.global_step)\n",
    "            tf_dict = {\n",
    "                self.x_tf:self.x,\n",
    "                self.u_tf:self.u,\n",
    "                self.u_deriv_tf:self.u_deri\n",
    "            }\n",
    "            _, loss_value, lo, lo_deriv = self.sess.run([self.optimizer,\n",
    "                self.loss, self.loss_res, self.loss_deriv], tf_dict)\n",
    "            self.sess.run(self.lr)\n",
    "            if iter % 2000 == 0:\n",
    "                print(\"第%d次 %f,res %f,deriv %f\"%(iter, loss_value, lo, lo_deriv))\n",
    "        print(\"第%d次的损失为%f\"%(max_iter, loss_value))\n",
    "        print(\"最终lr为%f\"%(self.sess.run(self.lr)))\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        u_star = self.sess.run(self.u_pred, {self.x_tf: X_star})\n",
    "        return u_star"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "layers = [1, 20, 20, 20, 20, 1]\n",
    "activation = tf.tanh\n",
    "lr = 1e-3\n",
    "weight_g = 0.1\n",
    "iterations = 10000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def train_model(train_num):\n",
    "    x = np.linspace(0, 1, train_num)[:, None]\n",
    "    u = -(1.4-3*x)*np.sin(18*x)\n",
    "    u_deriv = 3*np.sin(18*x) - (1.4-3*x)*18*np.cos(18*x)\n",
    "    start_time = time.time()\n",
    "    gNN_tanh = gNN(x, u, u_deriv, layers, activation, lr, weight_g)\n",
    "    print(\"Start training! train_num:%d\"%(train_num))\n",
    "    gNN_tanh.train(iterations)\n",
    "    elapsed = time.time() - start_time\n",
    "    print('Training time: %.4f' % (elapsed))\n",
    "    return gNN_tanh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def compute_error(model, test_num):\n",
    "    x_test   = np.linspace(0, 1, test_num)[:, None]\n",
    "    u_test   = -(1.4-3*x_test)*np.sin(18*x_test)\n",
    "    u_pred   = model.predict(x_test)\n",
    "    L2_norm  = np.linalg.norm(u_test-u_pred,2)/np.linalg.norm(u_test,2)\n",
    "    return L2_norm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training! train_num:5\n",
      "第0次 21.512825,res 0.485205,deriv 210.276199\n",
      "第2000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第4000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第6000次 0.000000,res 0.000000,deriv 0.000004\n",
      "第8000次 0.000001,res 0.000000,deriv 0.000005\n",
      "第10000次的损失为0.000000\n",
      "最终lr为0.000349\n",
      "Training time: 29.8202\n",
      "train_num:5, error:0.388964\n",
      "Start training! train_num:6\n",
      "第0次 18.330145,res 0.443336,deriv 178.868088\n",
      "第2000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第4000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第6000次 0.000001,res 0.000000,deriv 0.000008\n",
      "第8000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第10000次的损失为0.002375\n",
      "最终lr为0.000349\n",
      "Training time: 29.5429\n",
      "train_num:6, error:0.191889\n",
      "Start training! train_num:7\n",
      "第0次 21.246063,res 0.273608,deriv 209.724533\n",
      "第2000次 0.013452,res 0.012494,deriv 0.009586\n",
      "第4000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第6000次 0.000000,res 0.000000,deriv 0.000003\n",
      "第8000次 0.000291,res 0.000003,deriv 0.002873\n",
      "第10000次的损失为0.000000\n",
      "最终lr为0.000349\n",
      "Training time: 30.4271\n",
      "train_num:7, error:0.145917\n",
      "Start training! train_num:8\n",
      "第0次 23.300259,res 0.307690,deriv 229.925690\n",
      "第2000次 3.585175,res 0.030549,deriv 35.546257\n",
      "第4000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第6000次 0.000000,res 0.000000,deriv 0.000002\n",
      "第8000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第10000次的损失为0.000447\n",
      "最终lr为0.000349\n",
      "Training time: 28.8945\n",
      "train_num:8, error:0.328415\n",
      "Start training! train_num:9\n",
      "第0次 19.794584,res 0.382526,deriv 194.120575\n",
      "第2000次 0.002011,res 0.001876,deriv 0.001347\n",
      "第4000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第6000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第8000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第10000次的损失为0.000093\n",
      "最终lr为0.000349\n",
      "Training time: 33.9567\n",
      "train_num:9, error:0.091611\n",
      "Start training! train_num:10\n",
      "第0次 17.989708,res 0.354856,deriv 176.348511\n",
      "第2000次 0.000576,res 0.000472,deriv 0.001040\n",
      "第4000次 0.000004,res 0.000000,deriv 0.000039\n",
      "第6000次 0.000082,res 0.000004,deriv 0.000775\n",
      "第8000次 0.000007,res 0.000000,deriv 0.000070\n",
      "第10000次的损失为0.000000\n",
      "最终lr为0.000349\n",
      "Training time: 33.1038\n",
      "train_num:10, error:0.018125\n",
      "Start training! train_num:11\n",
      "第0次 17.112297,res 0.394131,deriv 167.181656\n",
      "第2000次 0.025085,res 0.024152,deriv 0.009324\n",
      "第4000次 0.000990,res 0.000435,deriv 0.005546\n",
      "第6000次 0.001511,res 0.000083,deriv 0.014279\n",
      "第8000次 0.000001,res 0.000001,deriv 0.000001\n",
      "第10000次的损失为0.000104\n",
      "最终lr为0.000349\n",
      "Training time: 33.0563\n",
      "train_num:11, error:0.013280\n",
      "Start training! train_num:12\n",
      "第0次 16.498434,res 0.441116,deriv 160.573166\n",
      "第2000次 2.352973,res 0.001112,deriv 23.518614\n",
      "第4000次 2.343531,res 0.001130,deriv 23.424009\n",
      "第6000次 0.000009,res 0.000008,deriv 0.000010\n",
      "第8000次 0.000001,res 0.000001,deriv 0.000001\n",
      "第10000次的损失为0.000001\n",
      "最终lr为0.000349\n",
      "Training time: 34.0913\n",
      "train_num:12, error:0.018835\n",
      "Start training! train_num:13\n",
      "第0次 16.062975,res 0.487562,deriv 155.754135\n",
      "第2000次 0.002943,res 0.002842,deriv 0.001013\n",
      "第4000次 0.000378,res 0.000331,deriv 0.000467\n",
      "第6000次 0.000098,res 0.000082,deriv 0.000153\n",
      "第8000次 0.000014,res 0.000011,deriv 0.000031\n",
      "第10000次的损失为0.000003\n",
      "最终lr为0.000349\n",
      "Training time: 33.4002\n",
      "train_num:13, error:0.005615\n",
      "Start training! train_num:14\n",
      "第0次 15.554363,res 0.438741,deriv 151.156219\n",
      "第2000次 0.000553,res 0.000524,deriv 0.000292\n",
      "第4000次 0.000042,res 0.000041,deriv 0.000019\n",
      "第6000次 0.000023,res 0.000018,deriv 0.000049\n",
      "第8000次 0.000012,res 0.000011,deriv 0.000012\n",
      "第10000次的损失为0.000010\n",
      "最终lr为0.000349\n",
      "Training time: 33.6513\n",
      "train_num:14, error:0.006794\n",
      "Start training! train_num:15\n",
      "第0次 15.163375,res 0.414013,deriv 147.493622\n",
      "第2000次 0.003212,res 0.003183,deriv 0.000297\n",
      "第4000次 0.001132,res 0.000823,deriv 0.003092\n",
      "第6000次 0.000272,res 0.000263,deriv 0.000091\n",
      "第8000次 0.000095,res 0.000089,deriv 0.000062\n",
      "第10000次的损失为0.000040\n",
      "最终lr为0.000349\n",
      "Training time: 33.2523\n",
      "train_num:15, error:0.009078\n",
      "Start training! train_num:16\n",
      "第0次 15.001759,res 0.466697,deriv 145.350616\n",
      "第2000次 0.000068,res 0.000067,deriv 0.000007\n",
      "第4000次 0.000051,res 0.000051,deriv 0.000006\n",
      "第6000次 0.000038,res 0.000038,deriv 0.000003\n",
      "第8000次 0.000028,res 0.000028,deriv 0.000001\n",
      "第10000次的损失为0.001632\n",
      "最终lr为0.000349\n",
      "Training time: 36.3037\n",
      "train_num:16, error:0.011152\n",
      "Start training! train_num:17\n",
      "第0次 14.702236,res 0.432441,deriv 142.697952\n",
      "第2000次 0.000325,res 0.000251,deriv 0.000738\n",
      "第4000次 0.000131,res 0.000097,deriv 0.000335\n",
      "第6000次 0.000052,res 0.000050,deriv 0.000016\n",
      "第8000次 0.000266,res 0.000052,deriv 0.002143\n",
      "第10000次的损失为0.000020\n",
      "最终lr为0.000349\n",
      "Training time: 34.0098\n",
      "train_num:17, error:0.004633\n",
      "Start training! train_num:18\n",
      "第0次 14.460127,res 0.412588,deriv 140.475388\n",
      "第2000次 0.001584,res 0.001542,deriv 0.000425\n",
      "第4000次 0.000483,res 0.000477,deriv 0.000063\n",
      "第6000次 0.000187,res 0.000185,deriv 0.000022\n",
      "第8000次 0.000080,res 0.000078,deriv 0.000011\n",
      "第10000次的损失为0.000038\n",
      "最终lr为0.000349\n",
      "Training time: 33.6224\n",
      "train_num:18, error:0.007013\n",
      "Start training! train_num:19\n",
      "第0次 14.207821,res 0.424142,deriv 137.836792\n",
      "第2000次 0.000878,res 0.000180,deriv 0.006984\n",
      "第4000次 0.000103,res 0.000099,deriv 0.000047\n",
      "第6000次 0.001337,res 0.000131,deriv 0.012065\n",
      "第8000次 0.000059,res 0.000057,deriv 0.000022\n",
      "第10000次的损失为0.000060\n",
      "最终lr为0.000349\n",
      "Training time: 33.3099\n",
      "train_num:19, error:0.007834\n",
      "Start training! train_num:20\n",
      "第0次 14.199433,res 0.438969,deriv 137.604645\n",
      "第2000次 0.001150,res 0.000744,deriv 0.004062\n",
      "第4000次 0.000284,res 0.000280,deriv 0.000041\n",
      "第6000次 0.000132,res 0.000130,deriv 0.000023\n",
      "第8000次 0.000667,res 0.000085,deriv 0.005819\n",
      "第10000次的损失为0.000036\n",
      "最终lr为0.000349\n",
      "Training time: 33.6243\n",
      "train_num:20, error:0.006822\n",
      "Start training! train_num:21\n",
      "第0次 13.979747,res 0.402407,deriv 135.773392\n",
      "第2000次 0.000197,res 0.000149,deriv 0.000485\n",
      "第4000次 0.000059,res 0.000057,deriv 0.000024\n",
      "第6000次 0.000539,res 0.000061,deriv 0.004779\n",
      "第8000次 0.000020,res 0.000019,deriv 0.000007\n",
      "第10000次的损失为0.000014\n",
      "最终lr为0.000349\n",
      "Training time: 33.7827\n",
      "train_num:21, error:0.003917\n",
      "Start training! train_num:22\n",
      "第0次 14.145089,res 0.546444,deriv 135.986450\n",
      "第2000次 0.000573,res 0.000069,deriv 0.005040\n",
      "第4000次 0.000054,res 0.000018,deriv 0.000356\n",
      "第6000次 0.000024,res 0.000009,deriv 0.000149\n",
      "第8000次 0.000007,res 0.000006,deriv 0.000010\n",
      "第10000次的损失为0.000004\n",
      "最终lr为0.000349\n",
      "Training time: 33.7225\n",
      "train_num:22, error:0.002243\n",
      "Start training! train_num:23\n",
      "第0次 13.820848,res 0.425645,deriv 133.952026\n",
      "第2000次 0.000062,res 0.000051,deriv 0.000103\n",
      "第4000次 0.000059,res 0.000040,deriv 0.000194\n",
      "第6000次 0.000034,res 0.000031,deriv 0.000024\n",
      "第8000次 0.000114,res 0.000025,deriv 0.000895\n",
      "第10000次的损失为0.000024\n",
      "最终lr为0.000349\n",
      "Training time: 33.8559\n",
      "train_num:23, error:0.004439\n",
      "Start training! train_num:24\n",
      "第0次 13.859617,res 0.494012,deriv 133.656052\n",
      "第2000次 0.000158,res 0.000086,deriv 0.000720\n",
      "第4000次 0.000151,res 0.000076,deriv 0.000756\n",
      "第6000次 0.000073,res 0.000065,deriv 0.000076\n",
      "第8000次 0.000063,res 0.000056,deriv 0.000078\n",
      "第10000次的损失为0.000047\n",
      "最终lr为0.000349\n",
      "Training time: 33.8694\n",
      "train_num:24, error:0.007578\n",
      "Start training! train_num:25\n",
      "第0次 13.747980,res 0.480228,deriv 132.677521\n",
      "第2000次 0.001213,res 0.000021,deriv 0.011921\n",
      "第4000次 0.000050,res 0.000008,deriv 0.000420\n",
      "第6000次 0.000020,res 0.000007,deriv 0.000124\n",
      "第8000次 0.000013,res 0.000006,deriv 0.000074\n",
      "第10000次的损失为0.000007\n",
      "最终lr为0.000349\n",
      "Training time: 33.7333\n",
      "train_num:25, error:0.002139\n",
      "Start training! train_num:26\n",
      "第0次 13.527727,res 0.409737,deriv 131.179901\n",
      "第2000次 0.001106,res 0.000021,deriv 0.010848\n",
      "第4000次 0.000372,res 0.000017,deriv 0.003549\n",
      "第6000次 0.000071,res 0.000004,deriv 0.000664\n",
      "第8000次 0.000006,res 0.000003,deriv 0.000029\n",
      "第10000次的损失为0.000005\n",
      "最终lr为0.000349\n",
      "Training time: 34.3516\n",
      "train_num:26, error:0.001265\n",
      "Start training! train_num:27\n",
      "第0次 13.555243,res 0.454793,deriv 131.004501\n",
      "第2000次 0.033369,res 0.001330,deriv 0.320386\n",
      "第4000次 0.005511,res 0.000113,deriv 0.053982\n",
      "第6000次 0.002774,res 0.000125,deriv 0.026491\n",
      "第8000次 0.000837,res 0.000016,deriv 0.008210\n",
      "第10000次的损失为0.003507\n",
      "最终lr为0.000349\n",
      "Training time: 35.8398\n",
      "train_num:27, error:0.029051\n",
      "Start training! train_num:28\n",
      "第0次 13.392251,res 0.409622,deriv 129.826294\n",
      "第2000次 0.000308,res 0.000001,deriv 0.003066\n",
      "第4000次 0.000051,res 0.000000,deriv 0.000513\n",
      "第6000次 0.000162,res 0.000041,deriv 0.001214\n",
      "第8000次 0.000018,res 0.000000,deriv 0.000182\n",
      "第10000次的损失为0.000200\n",
      "最终lr为0.000349\n",
      "Training time: 34.1920\n",
      "train_num:28, error:0.004926\n",
      "Start training! train_num:29\n",
      "第0次 13.321427,res 0.405371,deriv 129.160553\n",
      "第2000次 0.001475,res 0.000009,deriv 0.014656\n",
      "第4000次 0.000093,res 0.000005,deriv 0.000885\n",
      "第6000次 0.000039,res 0.000005,deriv 0.000341\n",
      "第8000次 0.000026,res 0.000004,deriv 0.000219\n",
      "第10000次的损失为0.000019\n",
      "最终lr为0.000349\n",
      "Training time: 34.1107\n",
      "train_num:29, error:0.001638\n",
      "Start training! train_num:30\n",
      "第0次 13.562803,res 0.562099,deriv 130.007034\n",
      "第2000次 0.000357,res 0.000001,deriv 0.003557\n",
      "第4000次 0.000035,res 0.000001,deriv 0.000340\n",
      "第6000次 0.001484,res 0.000032,deriv 0.014515\n",
      "第8000次 0.000059,res 0.000003,deriv 0.000552\n",
      "第10000次的损失为0.000008\n",
      "最终lr为0.000349\n",
      "Training time: 35.3667\n",
      "train_num:30, error:0.000764\n"
     ]
    }
   ],
   "source": [
    "train_nums = np.arange(5, 31, 1)\n",
    "test_num   = 1000\n",
    "errors     = []\n",
    "for train_num in train_nums:\n",
    "    model = train_model(train_num)\n",
    "    error = compute_error(model, test_num)\n",
    "    print(\"train_num:%d, error:%f\"%(train_num, error))\n",
    "    errors.append(error)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors = np.array(errors)\n",
    "plt.plot(train_nums, errors, 'b-')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "np.save('errors.npy', errors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "weight_g = 0.01"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training! train_num:5\n",
      "第0次 2.547623,res 0.442225,deriv 210.539795\n",
      "第2000次 0.000005,res 0.000005,deriv 0.000061\n",
      "第4000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第6000次 0.000000,res 0.000000,deriv 0.000015\n",
      "第8000次 0.000155,res 0.000033,deriv 0.012228\n",
      "第10000次的损失为0.000028\n",
      "最终lr为0.000349\n",
      "Training time: 34.1053\n",
      "train_num:5, error:0.596332\n",
      "Start training! train_num:6\n",
      "第0次 2.285063,res 0.491443,deriv 179.361984\n",
      "第2000次 0.000156,res 0.000146,deriv 0.000964\n",
      "第4000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第6000次 0.000035,res 0.000027,deriv 0.000804\n",
      "第8000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第10000次的损失为0.000000\n",
      "最终lr为0.000349\n",
      "Training time: 34.4934\n",
      "train_num:6, error:0.177567\n",
      "Start training! train_num:7\n",
      "第0次 2.395401,res 0.290073,deriv 210.532822\n",
      "第2000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第4000次 0.000368,res 0.000082,deriv 0.028649\n",
      "第6000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第8000次 0.000000,res 0.000000,deriv 0.000001\n",
      "第10000次的损失为0.000049\n",
      "最终lr为0.000349\n",
      "Training time: 34.4353\n",
      "train_num:7, error:0.299706\n",
      "Start training! train_num:8\n",
      "第0次 2.604964,res 0.305722,deriv 229.924225\n",
      "第2000次 0.001831,res 0.000648,deriv 0.118330\n",
      "第4000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第6000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第8000次 0.000002,res 0.000001,deriv 0.000084\n",
      "第10000次的损失为0.000000\n",
      "最终lr为0.000349\n",
      "Training time: 34.9843\n",
      "train_num:8, error:0.115628\n",
      "Start training! train_num:9\n",
      "第0次 2.265258,res 0.331856,deriv 193.340164\n",
      "第2000次 0.000075,res 0.000027,deriv 0.004801\n",
      "第4000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第6000次 0.000000,res 0.000000,deriv 0.000004\n",
      "第8000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第10000次的损失为0.000029\n",
      "最终lr为0.000349\n",
      "Training time: 34.7850\n",
      "train_num:9, error:0.083366\n",
      "Start training! train_num:10\n",
      "第0次 2.211092,res 0.433193,deriv 177.789917\n",
      "第2000次 0.001313,res 0.000379,deriv 0.093342\n",
      "第4000次 0.000409,res 0.000230,deriv 0.017900\n",
      "第6000次 0.000000,res 0.000000,deriv 0.000007\n",
      "第8000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第10000次的损失为0.000000\n",
      "最终lr为0.000349\n",
      "Training time: 34.7046\n",
      "train_num:10, error:0.020504\n",
      "Start training! train_num:11\n",
      "第0次 2.038892,res 0.371405,deriv 166.748642\n",
      "第2000次 0.000092,res 0.000032,deriv 0.006006\n",
      "第4000次 0.000007,res 0.000002,deriv 0.000534\n",
      "第6000次 0.000000,res 0.000000,deriv 0.000013\n",
      "第8000次 0.000000,res 0.000000,deriv 0.000004\n",
      "第10000次的损失为0.000000\n",
      "最终lr为0.000349\n",
      "Training time: 34.9368\n",
      "train_num:11, error:0.055333\n",
      "Start training! train_num:12\n",
      "第0次 2.034268,res 0.429178,deriv 160.509033\n",
      "第2000次 0.000002,res 0.000001,deriv 0.000065\n",
      "第4000次 0.000002,res 0.000001,deriv 0.000172\n",
      "第6000次 0.000000,res 0.000000,deriv 0.000000\n",
      "第8000次 0.000002,res 0.000001,deriv 0.000151\n",
      "第10000次的损失为0.000000\n",
      "最终lr为0.000349\n",
      "Training time: 34.8931\n",
      "train_num:12, error:0.013040\n",
      "Start training! train_num:13\n",
      "第0次 1.925923,res 0.382740,deriv 154.318298\n",
      "第2000次 0.000415,res 0.000355,deriv 0.006025\n",
      "第4000次 0.000096,res 0.000081,deriv 0.001543\n",
      "第6000次 0.000048,res 0.000038,deriv 0.000983\n",
      "第8000次 0.000028,res 0.000022,deriv 0.000577\n",
      "第10000次的损失为0.000017\n",
      "最终lr为0.000349\n",
      "Training time: 41.5028\n",
      "train_num:13, error:0.006164\n",
      "Start training! train_num:14\n",
      "第0次 2.011337,res 0.494814,deriv 151.652313\n",
      "第2000次 0.032709,res 0.007130,deriv 2.557966\n",
      "第4000次 0.000072,res 0.000052,deriv 0.002058\n",
      "第6000次 0.000018,res 0.000012,deriv 0.000603\n",
      "第8000次 0.000027,res 0.000010,deriv 0.001645\n",
      "第10000次的损失为0.000006\n",
      "最终lr为0.000349\n",
      "Training time: 39.4829\n",
      "train_num:14, error:0.004235\n",
      "Start training! train_num:15\n",
      "第0次 1.890594,res 0.415444,deriv 147.515045\n",
      "第2000次 0.000035,res 0.000017,deriv 0.001793\n",
      "第4000次 0.000007,res 0.000006,deriv 0.000079\n",
      "第6000次 0.000004,res 0.000003,deriv 0.000105\n",
      "第8000次 0.000002,res 0.000002,deriv 0.000033\n",
      "第10000次的损失为0.000002\n",
      "最终lr为0.000349\n",
      "Training time: 41.2493\n",
      "train_num:15, error:0.006778\n",
      "Start training! train_num:16\n",
      "第0次 1.866447,res 0.418053,deriv 144.839401\n",
      "第2000次 0.195349,res 0.004358,deriv 19.099186\n",
      "第4000次 0.003717,res 0.003273,deriv 0.044429\n",
      "第6000次 0.000029,res 0.000023,deriv 0.000565\n",
      "第8000次 0.000008,res 0.000006,deriv 0.000128\n",
      "第10000次的损失为0.000004\n",
      "最终lr为0.000349\n",
      "Training time: 39.2584\n",
      "train_num:16, error:0.007174\n",
      "Start training! train_num:17\n",
      "第0次 1.848466,res 0.422675,deriv 142.579117\n",
      "第2000次 0.000257,res 0.000224,deriv 0.003369\n",
      "第4000次 0.000053,res 0.000050,deriv 0.000258\n",
      "第6000次 0.000024,res 0.000022,deriv 0.000221\n",
      "第8000次 0.000017,res 0.000015,deriv 0.000167\n",
      "第10000次的损失为0.000017\n",
      "最终lr为0.000349\n",
      "Training time: 45.0175\n",
      "train_num:17, error:0.004383\n",
      "Start training! train_num:18\n",
      "第0次 1.866899,res 0.457123,deriv 140.977631\n",
      "第2000次 0.000600,res 0.000547,deriv 0.005326\n",
      "第4000次 0.000083,res 0.000076,deriv 0.000719\n",
      "第6000次 0.000016,res 0.000012,deriv 0.000396\n",
      "第8000次 0.000090,res 0.000031,deriv 0.005892\n",
      "第10000次的损失为0.000271\n",
      "最终lr为0.000349\n",
      "Training time: 42.2700\n",
      "train_num:18, error:0.017295\n",
      "Start training! train_num:19\n",
      "第0次 1.948437,res 0.548531,deriv 139.990601\n",
      "第2000次 0.000743,res 0.000669,deriv 0.007346\n",
      "第4000次 0.001537,res 0.000282,deriv 0.125555\n",
      "第6000次 0.000143,res 0.000129,deriv 0.001431\n",
      "第8000次 0.000169,res 0.000054,deriv 0.011586\n",
      "第10000次的损失为0.000031\n",
      "最终lr为0.000349\n",
      "Training time: 37.7099\n",
      "train_num:19, error:0.005699\n",
      "Start training! train_num:20\n",
      "第0次 1.762985,res 0.395874,deriv 136.711090\n",
      "第2000次 0.000166,res 0.000143,deriv 0.002220\n",
      "第4000次 0.000015,res 0.000015,deriv 0.000076\n",
      "第6000次 0.000006,res 0.000006,deriv 0.000053\n",
      "第8000次 0.000004,res 0.000003,deriv 0.000033\n",
      "第10000次的损失为0.000008\n",
      "最终lr为0.000349\n",
      "Training time: 40.1356\n",
      "train_num:20, error:0.002978\n",
      "Start training! train_num:21\n",
      "第0次 1.752970,res 0.397680,deriv 135.529068\n",
      "第2000次 0.001656,res 0.000532,deriv 0.112468\n",
      "第4000次 0.000073,res 0.000018,deriv 0.005563\n",
      "第6000次 0.000101,res 0.000026,deriv 0.007521\n",
      "第8000次 0.000029,res 0.000006,deriv 0.002355\n",
      "第10000次的损失为0.000015\n",
      "最终lr为0.000349\n",
      "Training time: 38.0127\n",
      "train_num:21, error:0.002071\n",
      "Start training! train_num:22\n",
      "第0次 1.835464,res 0.480361,deriv 135.510239\n",
      "第2000次 0.000647,res 0.000545,deriv 0.010291\n",
      "第4000次 0.000193,res 0.000172,deriv 0.002109\n",
      "第6000次 0.001976,res 0.000432,deriv 0.154410\n",
      "第8000次 0.000041,res 0.000038,deriv 0.000276\n",
      "第10000次的损失为0.000023\n",
      "最终lr为0.000349\n",
      "Training time: 42.4089\n",
      "train_num:22, error:0.005180\n",
      "Start training! train_num:23\n",
      "第0次 1.783067,res 0.441932,deriv 134.113541\n",
      "第2000次 0.000042,res 0.000004,deriv 0.003807\n",
      "第4000次 0.000009,res 0.000003,deriv 0.000567\n",
      "第6000次 0.000005,res 0.000003,deriv 0.000224\n",
      "第8000次 0.000003,res 0.000002,deriv 0.000139\n",
      "第10000次的损失为0.000007\n",
      "最终lr为0.000349\n",
      "Training time: 41.8423\n",
      "train_num:23, error:0.002577\n",
      "Start training! train_num:24\n",
      "第0次 1.738748,res 0.410766,deriv 132.798157\n",
      "第2000次 0.000243,res 0.000134,deriv 0.010864\n",
      "第4000次 0.000067,res 0.000035,deriv 0.003137\n",
      "第6000次 0.000020,res 0.000010,deriv 0.000985\n",
      "第8000次 0.000013,res 0.000005,deriv 0.000781\n",
      "第10000次的损失为0.000010\n",
      "最终lr为0.000349\n",
      "Training time: 41.3626\n",
      "train_num:24, error:0.002216\n",
      "Start training! train_num:25\n",
      "第0次 1.718216,res 0.405289,deriv 131.292709\n",
      "第2000次 0.007962,res 0.000205,deriv 0.775695\n",
      "第4000次 0.000086,res 0.000019,deriv 0.006641\n",
      "第6000次 0.000019,res 0.000008,deriv 0.001105\n",
      "第8000次 0.000009,res 0.000005,deriv 0.000399\n",
      "第10000次的损失为0.000045\n",
      "最终lr为0.000349\n",
      "Training time: 39.7110\n",
      "train_num:25, error:0.004597\n",
      "Start training! train_num:26\n",
      "第0次 1.715214,res 0.404503,deriv 131.071030\n",
      "第2000次 0.389842,res 0.034108,deriv 35.573376\n",
      "第4000次 0.000567,res 0.000099,deriv 0.046810\n",
      "第6000次 0.000243,res 0.000044,deriv 0.019848\n",
      "第8000次 0.000107,res 0.000024,deriv 0.008265\n",
      "第10000次的损失为0.000049\n",
      "最终lr为0.000349\n",
      "Training time: 37.3053\n",
      "train_num:26, error:0.004138\n",
      "Start training! train_num:27\n",
      "第0次 1.707147,res 0.403674,deriv 130.347366\n",
      "第2000次 0.002303,res 0.000728,deriv 0.157514\n",
      "第4000次 0.000040,res 0.000006,deriv 0.003426\n",
      "第6000次 0.000021,res 0.000003,deriv 0.001837\n",
      "第8000次 0.000664,res 0.000307,deriv 0.035686\n",
      "第10000次的损失为0.000161\n",
      "最终lr为0.000349\n",
      "Training time: 41.6581\n",
      "train_num:27, error:0.019722\n",
      "Start training! train_num:28\n",
      "第0次 1.727718,res 0.426979,deriv 130.073822\n",
      "第2000次 1.511081,res 0.307794,deriv 120.328720\n",
      "第4000次 0.001723,res 0.000070,deriv 0.165303\n",
      "第6000次 0.000262,res 0.000009,deriv 0.025270\n",
      "第8000次 0.000105,res 0.000003,deriv 0.010174\n",
      "第10000次的损失为0.000059\n",
      "最终lr为0.000349\n",
      "Training time: 40.6846\n",
      "train_num:28, error:0.002095\n",
      "Start training! train_num:29\n",
      "第0次 1.730833,res 0.435060,deriv 129.577301\n",
      "第2000次 0.000151,res 0.000024,deriv 0.012722\n",
      "第4000次 0.000050,res 0.000010,deriv 0.004051\n",
      "第6000次 0.000017,res 0.000003,deriv 0.001382\n",
      "第8000次 0.000007,res 0.000001,deriv 0.000566\n",
      "第10000次的损失为0.000008\n",
      "最终lr为0.000349\n",
      "Training time: 38.2014\n",
      "train_num:29, error:0.002377\n",
      "Start training! train_num:30\n",
      "第0次 1.685432,res 0.400671,deriv 128.476059\n",
      "第2000次 0.000208,res 0.000060,deriv 0.014815\n",
      "第4000次 0.000326,res 0.000084,deriv 0.024196\n",
      "第6000次 0.000054,res 0.000011,deriv 0.004265\n",
      "第8000次 0.000012,res 0.000004,deriv 0.000803\n",
      "第10000次的损失为0.000001\n",
      "最终lr为0.000349\n",
      "Training time: 37.5573\n",
      "train_num:30, error:0.000861\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors     = []\n",
    "for train_num in train_nums:\n",
    "    model = train_model(train_num)\n",
    "    error = compute_error(model, test_num)\n",
    "    print(\"train_num:%d, error:%f\"%(train_num, error))\n",
    "    errors.append(error)\n",
    "\n",
    "errors = np.array(errors)\n",
    "plt.plot(train_nums, errors, 'b-')\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "np.save('errors.npy', errors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}