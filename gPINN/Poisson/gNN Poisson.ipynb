{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "d:\\programfile\\python3.7.4\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # 这一行注释掉就是使用gpu，不注释就是使用cpu\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class gNN:\n",
    "    def __init__(self, x, layers, activation, lr, weight_g):\n",
    "        self.x = x\n",
    "        self.lr = lr\n",
    "        self.activation = activation\n",
    "        self.weight_g = weight_g\n",
    "        self.loss_log = []\n",
    "        self.loss_res_log = []\n",
    "        self.loss_deriv_log = []\n",
    "\n",
    "        self.weights, self.biases = self.initilize_NN(layers)\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "        self.x_tf = tf.placeholder(tf.float32, shape=[None, self.x.shape[1]])\n",
    "\n",
    "        self.surrogate_u = self.surrogate(self.x_tf)\n",
    "        self.residual    = self.residual(self.surrogate_u, self.x_tf)\n",
    "        self.loss_res    = tf.reduce_mean(tf.square(self.residual))\n",
    "        self.loss_deri   = tf.reduce_mean(tf.square(self.resi_deriv(self.residual, self.x_tf)))\n",
    "        self.loss        = self.loss_res + self.weight_g * self.loss_deri\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    def initilize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases  = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        return weights, biases\n",
    "\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "\n",
    "    def neural_net(self, X, weights, biases, activation):\n",
    "        num_layers = len(weights) + 1\n",
    "        H = X\n",
    "        for l in range(0, num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = activation(tf.add(tf.matmul(H, W), b))\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "\n",
    "    def net_u(self, x):\n",
    "        u = self.neural_net(x, self.weights, self.biases, self.activation)\n",
    "        return u\n",
    "\n",
    "    def surrogate(self, x):\n",
    "        return x*(np.pi - x)*self.net_u(x) + x\n",
    "\n",
    "    def residual(self, u, x):\n",
    "        u_x  = tf.gradients(u, x)[0]\n",
    "        u_xx = tf.gradients(u_x, x)[0]\n",
    "        rhs  = 0\n",
    "        for i in range(1, 5):\n",
    "            rhs += i*tf.sin(i*x)\n",
    "        rhs += 8*tf.sin(8*x)\n",
    "        return u_xx + rhs\n",
    "\n",
    "    def resi_deriv(self, res, x):\n",
    "        res_x = tf.gradients(res, x)[0]\n",
    "        return res_x\n",
    "\n",
    "    def callback(self, loss, res, deriv):\n",
    "        print('Loss:%f,res:%f,deriv:%f'%(loss, res, deriv))\n",
    "\n",
    "    def train(self, max_iter=40000):\n",
    "        loss_value = np.inf\n",
    "        for iter in range(max_iter):\n",
    "            tf_dict = {\n",
    "                self.x_tf:self.x\n",
    "            }\n",
    "            _, loss_value, lo_res, lo_deriv = self.sess.run([self.optimizer,\n",
    "                self.loss, self.loss_res, self.loss_deri], tf_dict)\n",
    "            if iter % 1000 == 0:\n",
    "                print(\"第%d次 %f,res %f,deriv %f\"%(iter, loss_value, lo_res, lo_deriv))\n",
    "        print(\"第%d次的损失为%f\"%(max_iter, loss_value))\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        u_star = self.sess.run(self.surrogate_u, {self.x_tf: X_star})\n",
    "        return u_star"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "layers = [1, 20, 20, 20, 20, 1]\n",
    "activation = tf.tanh\n",
    "lr = 1e-3\n",
    "weight_g = 1\n",
    "iterations = 20000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def train_model(train_num):\n",
    "    x = np.linspace(0, np.pi, train_num)[:, None]\n",
    "    start_time = time.time()\n",
    "    gNN_tanh = gNN(x, layers, activation, lr, weight_g)\n",
    "    print(\"Start training! train_num:%d\"%(train_num))\n",
    "    gNN_tanh.train(iterations)\n",
    "    elapsed = time.time() - start_time\n",
    "    print('Training time: %.4f' % (elapsed))\n",
    "    return gNN_tanh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def compute_error(model, test_num):\n",
    "    x_test   = np.linspace(0, np.pi, test_num)[:, None]\n",
    "    u_test   = x_test\n",
    "    for i in range(1, 5):\n",
    "        u_test += np.sin(i*x_test) / i\n",
    "    u_test  +=  np.sin(8*x_test)/8\n",
    "    u_pred   = model.predict(x_test)\n",
    "    L2_norm  = np.linalg.norm(u_test-u_pred,2)/np.linalg.norm(u_test,2)\n",
    "    return L2_norm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training! train_num:10\n",
      "第0次 2762.393555,res 41.445503,deriv 2720.947998\n",
      "第1000次 1193.260864,res 17.184135,deriv 1176.076782\n",
      "第2000次 968.923706,res 14.549777,deriv 954.373901\n",
      "第3000次 4.423996,res 0.443858,deriv 3.980138\n",
      "第4000次 0.000308,res 0.000114,deriv 0.000194\n",
      "第5000次 0.045208,res 0.026935,deriv 0.018273\n",
      "第6000次 0.008901,res 0.006367,deriv 0.002534\n",
      "第7000次 0.023326,res 0.001310,deriv 0.022016\n",
      "第8000次 0.038697,res 0.006928,deriv 0.031769\n",
      "第9000次 0.498456,res 0.005853,deriv 0.492603\n",
      "第10000次 0.043448,res 0.003377,deriv 0.040071\n",
      "第11000次 1.595584,res 0.009813,deriv 1.585771\n",
      "第12000次 0.419789,res 0.018927,deriv 0.400863\n",
      "第13000次 0.150977,res 0.006751,deriv 0.144226\n",
      "第14000次 0.081023,res 0.005293,deriv 0.075730\n",
      "第15000次 0.250766,res 0.028284,deriv 0.222483\n",
      "第16000次 0.021832,res 0.001568,deriv 0.020264\n",
      "第17000次 0.223306,res 0.018731,deriv 0.204574\n",
      "第18000次 0.018318,res 0.001904,deriv 0.016415\n",
      "第19000次 0.013748,res 0.000821,deriv 0.012927\n",
      "第20000次的损失为0.050226\n",
      "Training time: 111.7675\n",
      "train_num:10, error:0.372403\n",
      "Start training! train_num:11\n",
      "第0次 2716.185791,res 42.182758,deriv 2674.002930\n",
      "第1000次 749.845581,res 14.814023,deriv 735.031555\n",
      "第2000次 110.488327,res 60.944515,deriv 49.543812\n",
      "第3000次 5.405949,res 5.333279,deriv 0.072669\n",
      "第4000次 0.002562,res 0.000645,deriv 0.001916\n",
      "第5000次 0.002048,res 0.001299,deriv 0.000749\n",
      "第6000次 0.221302,res 0.013166,deriv 0.208136\n",
      "第7000次 0.531555,res 0.007895,deriv 0.523660\n",
      "第8000次 0.078677,res 0.004041,deriv 0.074636\n",
      "第9000次 0.086471,res 0.001726,deriv 0.084745\n",
      "第10000次 0.060977,res 0.000551,deriv 0.060426\n",
      "第11000次 0.988354,res 0.019000,deriv 0.969354\n",
      "第12000次 0.136203,res 0.004157,deriv 0.132046\n",
      "第13000次 0.150000,res 0.013034,deriv 0.136966\n",
      "第14000次 0.105906,res 0.002582,deriv 0.103324\n",
      "第15000次 0.183005,res 0.004199,deriv 0.178806\n",
      "第16000次 0.146418,res 0.010596,deriv 0.135821\n",
      "第17000次 0.039947,res 0.002273,deriv 0.037674\n",
      "第18000次 0.029827,res 0.000557,deriv 0.029270\n",
      "第19000次 0.028238,res 0.016720,deriv 0.011518\n",
      "第20000次的损失为0.091485\n",
      "Training time: 71.1869\n",
      "train_num:11, error:0.372929\n",
      "Start training! train_num:12\n",
      "第0次 2679.882812,res 42.704086,deriv 2637.178711\n",
      "第1000次 605.039001,res 23.085260,deriv 581.953735\n",
      "第2000次 4.303210,res 3.416519,deriv 0.886691\n",
      "第3000次 0.117732,res 0.111392,deriv 0.006339\n",
      "第4000次 0.021386,res 0.018689,deriv 0.002697\n",
      "第5000次 0.016587,res 0.014709,deriv 0.001877\n",
      "第6000次 0.010660,res 0.009236,deriv 0.001424\n",
      "第7000次 0.028277,res 0.021816,deriv 0.006461\n",
      "第8000次 0.007216,res 0.005104,deriv 0.002111\n",
      "第9000次 0.109238,res 0.030832,deriv 0.078406\n",
      "第10000次 0.067917,res 0.005151,deriv 0.062766\n",
      "第11000次 2.378322,res 0.051043,deriv 2.327279\n",
      "第12000次 0.014755,res 0.013716,deriv 0.001040\n",
      "第13000次 0.003678,res 0.002666,deriv 0.001012\n",
      "第14000次 0.071530,res 0.032583,deriv 0.038946\n",
      "第15000次 0.020871,res 0.016687,deriv 0.004184\n",
      "第16000次 0.007193,res 0.006427,deriv 0.000766\n",
      "第17000次 0.049747,res 0.024648,deriv 0.025099\n",
      "第18000次 0.095122,res 0.034809,deriv 0.060312\n",
      "第19000次 0.004274,res 0.003929,deriv 0.000344\n",
      "第20000次的损失为0.013631\n",
      "Training time: 63.0529\n",
      "train_num:12, error:0.345627\n",
      "Start training! train_num:13\n",
      "第0次 2646.991211,res 43.896065,deriv 2603.095215\n",
      "第1000次 655.607788,res 16.962929,deriv 638.644836\n",
      "第2000次 60.328819,res 40.117752,deriv 20.211067\n",
      "第3000次 0.131288,res 0.127867,deriv 0.003421\n",
      "第4000次 0.026771,res 0.025192,deriv 0.001578\n",
      "第5000次 0.010639,res 0.009092,deriv 0.001547\n",
      "第6000次 0.488934,res 0.021020,deriv 0.467914\n",
      "第7000次 0.006753,res 0.006330,deriv 0.000423\n",
      "第8000次 0.010862,res 0.009800,deriv 0.001062\n",
      "第9000次 0.005348,res 0.005168,deriv 0.000181\n",
      "第10000次 0.005643,res 0.005473,deriv 0.000171\n",
      "第11000次 0.008953,res 0.008715,deriv 0.000238\n",
      "第12000次 0.006867,res 0.006653,deriv 0.000214\n",
      "第13000次 0.007109,res 0.006879,deriv 0.000230\n",
      "第14000次 0.006938,res 0.006707,deriv 0.000232\n",
      "第15000次 0.005218,res 0.005023,deriv 0.000195\n",
      "第16000次 0.023633,res 0.014273,deriv 0.009360\n",
      "第17000次 0.009259,res 0.008906,deriv 0.000353\n",
      "第18000次 0.009230,res 0.008541,deriv 0.000689\n",
      "第19000次 0.006954,res 0.006701,deriv 0.000253\n",
      "第20000次的损失为0.004672\n",
      "Training time: 58.6609\n",
      "train_num:13, error:0.354996\n",
      "Start training! train_num:14\n",
      "第0次 2620.496338,res 43.896511,deriv 2576.599854\n",
      "第1000次 752.576355,res 10.671678,deriv 741.904663\n",
      "第2000次 45.770905,res 33.515572,deriv 12.255335\n",
      "第3000次 0.290324,res 0.284098,deriv 0.006226\n",
      "第4000次 0.085812,res 0.083964,deriv 0.001848\n",
      "第5000次 0.097038,res 0.040213,deriv 0.056825\n",
      "第6000次 0.044898,res 0.042202,deriv 0.002696\n",
      "第7000次 0.205436,res 0.055266,deriv 0.150170\n",
      "第8000次 0.054140,res 0.034172,deriv 0.019968\n",
      "第9000次 0.022659,res 0.021997,deriv 0.000662\n",
      "第10000次 0.064478,res 0.031054,deriv 0.033424\n",
      "第11000次 0.227977,res 0.019864,deriv 0.208114\n",
      "第12000次 0.017891,res 0.017262,deriv 0.000629\n",
      "第13000次 0.019071,res 0.018036,deriv 0.001035\n",
      "第14000次 0.017138,res 0.016518,deriv 0.000620\n",
      "第15000次 0.016808,res 0.016155,deriv 0.000653\n",
      "第16000次 0.016367,res 0.015683,deriv 0.000684\n",
      "第17000次 0.016119,res 0.015452,deriv 0.000666\n",
      "第18000次 0.019522,res 0.017129,deriv 0.002393\n",
      "第19000次 0.014243,res 0.013586,deriv 0.000657\n",
      "第20000次的损失为0.014916\n",
      "Training time: 54.9119\n",
      "train_num:14, error:0.347699\n",
      "Start training! train_num:15\n",
      "第0次 2597.338379,res 43.412739,deriv 2553.925537\n",
      "第1000次 409.903809,res 24.616314,deriv 385.287506\n",
      "第2000次 39.460106,res 27.834963,deriv 11.625142\n",
      "第3000次 0.492918,res 0.478330,deriv 0.014588\n",
      "第4000次 0.104730,res 0.101020,deriv 0.003710\n",
      "第5000次 0.540225,res 0.037331,deriv 0.502894\n",
      "第6000次 0.171510,res 0.027036,deriv 0.144474\n",
      "第7000次 0.538972,res 0.026270,deriv 0.512702\n",
      "第8000次 0.014196,res 0.013734,deriv 0.000462\n",
      "第9000次 0.016351,res 0.015899,deriv 0.000451\n",
      "第10000次 0.278378,res 0.026510,deriv 0.251868\n",
      "第11000次 0.016300,res 0.015916,deriv 0.000384\n",
      "第12000次 0.024967,res 0.020320,deriv 0.004647\n",
      "第13000次 0.012827,res 0.012523,deriv 0.000304\n",
      "第14000次 0.015660,res 0.015286,deriv 0.000374\n",
      "第15000次 0.013029,res 0.012694,deriv 0.000335\n",
      "第16000次 0.018895,res 0.018330,deriv 0.000564\n",
      "第17000次 0.015477,res 0.015087,deriv 0.000390\n",
      "第18000次 0.014523,res 0.014169,deriv 0.000354\n",
      "第19000次 0.011878,res 0.011565,deriv 0.000312\n",
      "第20000次的损失为0.013935\n",
      "Training time: 53.6484\n",
      "train_num:15, error:0.352296\n",
      "Start training! train_num:16\n",
      "第0次 2577.640381,res 42.175079,deriv 2535.465332\n",
      "第1000次 577.306458,res 11.629699,deriv 565.676758\n",
      "第2000次 0.992550,res 0.979130,deriv 0.013420\n",
      "第3000次 0.229231,res 0.225820,deriv 0.003411\n",
      "第4000次 0.619735,res 0.061324,deriv 0.558411\n",
      "第5000次 0.030621,res 0.030010,deriv 0.000611\n",
      "第6000次 0.086332,res 0.023299,deriv 0.063033\n",
      "第7000次 0.012273,res 0.011938,deriv 0.000335\n",
      "第8000次 0.010901,res 0.010725,deriv 0.000175\n",
      "第9000次 0.012234,res 0.011595,deriv 0.000640\n",
      "第10000次 4.973807,res 0.132545,deriv 4.841262\n",
      "第11000次 1.103091,res 0.017690,deriv 1.085401\n",
      "第12000次 3.677349,res 0.102484,deriv 3.574865\n",
      "第13000次 0.216131,res 0.021802,deriv 0.194329\n",
      "第14000次 0.017883,res 0.012773,deriv 0.005110\n",
      "第15000次 3.264099,res 0.093349,deriv 3.170750\n",
      "第16000次 0.012082,res 0.011868,deriv 0.000214\n",
      "第17000次 0.017495,res 0.014628,deriv 0.002867\n",
      "第18000次 0.012456,res 0.012206,deriv 0.000249\n",
      "第19000次 0.013267,res 0.012693,deriv 0.000574\n",
      "第20000次的损失为0.015042\n",
      "Training time: 53.9676\n",
      "train_num:16, error:0.351163\n",
      "Start training! train_num:17\n",
      "第0次 2562.466309,res 41.990437,deriv 2520.475830\n",
      "第1000次 456.795502,res 12.821575,deriv 443.973938\n",
      "第2000次 0.987194,res 0.969464,deriv 0.017730\n",
      "第3000次 0.093129,res 0.089592,deriv 0.003537\n",
      "第4000次 0.018276,res 0.016986,deriv 0.001289\n",
      "第5000次 0.005298,res 0.004805,deriv 0.000493\n",
      "第6000次 0.003589,res 0.003093,deriv 0.000496\n",
      "第7000次 0.002968,res 0.002747,deriv 0.000221\n",
      "第8000次 1.012624,res 0.026393,deriv 0.986231\n",
      "第9000次 0.007307,res 0.006139,deriv 0.001168\n",
      "第10000次 0.002510,res 0.002322,deriv 0.000188\n",
      "第11000次 0.003072,res 0.002800,deriv 0.000271\n",
      "第12000次 0.469606,res 0.016109,deriv 0.453497\n",
      "第13000次 0.003710,res 0.003316,deriv 0.000394\n",
      "第14000次 4.017087,res 0.079750,deriv 3.937336\n",
      "第15000次 0.005609,res 0.003352,deriv 0.002257\n",
      "第16000次 0.004220,res 0.003756,deriv 0.000464\n",
      "第17000次 0.175543,res 0.016027,deriv 0.159516\n",
      "第18000次 0.004744,res 0.004240,deriv 0.000504\n",
      "第19000次 0.009286,res 0.007315,deriv 0.001971\n",
      "第20000次的损失为0.004623\n",
      "Training time: 62.7097\n",
      "train_num:17, error:0.347388\n",
      "Start training! train_num:18\n",
      "第0次 2543.245361,res 44.625530,deriv 2498.619873\n",
      "第1000次 489.088562,res 17.669823,deriv 471.418732\n",
      "第2000次 191.374527,res 3.548736,deriv 187.825790\n",
      "第3000次 21.990702,res 18.552528,deriv 3.438174\n",
      "第4000次 22.723230,res 16.925138,deriv 5.798092\n",
      "第5000次 17.199722,res 14.494904,deriv 2.704818\n",
      "第6000次 0.082497,res 0.072083,deriv 0.010414\n",
      "第7000次 0.031857,res 0.029557,deriv 0.002301\n",
      "第8000次 0.019081,res 0.017925,deriv 0.001155\n",
      "第9000次 0.160736,res 0.015745,deriv 0.144990\n",
      "第10000次 0.012884,res 0.012334,deriv 0.000550\n",
      "第11000次 0.010840,res 0.010463,deriv 0.000377\n",
      "第12000次 0.009128,res 0.008838,deriv 0.000290\n",
      "第13000次 0.007877,res 0.007542,deriv 0.000336\n",
      "第14000次 0.204273,res 0.012668,deriv 0.191605\n",
      "第15000次 0.008510,res 0.007871,deriv 0.000639\n",
      "第16000次 0.005703,res 0.005533,deriv 0.000170\n",
      "第17000次 1.464550,res 0.033529,deriv 1.431021\n",
      "第18000次 0.004941,res 0.004783,deriv 0.000158\n",
      "第19000次 0.005114,res 0.003747,deriv 0.001368\n",
      "第20000次的损失为0.003955\n",
      "Training time: 57.9359\n",
      "train_num:18, error:0.348206\n",
      "Start training! train_num:19\n",
      "第0次 2530.222168,res 43.075726,deriv 2487.146484\n",
      "第1000次 1.056201,res 0.899380,deriv 0.156821\n",
      "第2000次 0.111045,res 0.107772,deriv 0.003273\n",
      "第3000次 0.036336,res 0.035395,deriv 0.000941\n",
      "第4000次 0.015219,res 0.014683,deriv 0.000536\n",
      "第5000次 0.007982,res 0.007653,deriv 0.000329\n",
      "第6000次 0.004628,res 0.004468,deriv 0.000160\n",
      "第7000次 0.003551,res 0.003136,deriv 0.000416\n",
      "第8000次 0.014944,res 0.002719,deriv 0.012225\n",
      "第9000次 0.001684,res 0.001649,deriv 0.000035\n",
      "第10000次 0.006543,res 0.001561,deriv 0.004982\n",
      "第11000次 0.000975,res 0.000944,deriv 0.000031\n",
      "第12000次 0.004494,res 0.001295,deriv 0.003199\n",
      "第13000次 0.000934,res 0.000890,deriv 0.000044\n",
      "第14000次 0.001152,res 0.001113,deriv 0.000038\n",
      "第15000次 0.001675,res 0.001569,deriv 0.000106\n",
      "第16000次 0.002177,res 0.001796,deriv 0.000381\n",
      "第17000次 0.001698,res 0.001636,deriv 0.000062\n",
      "第18000次 0.003239,res 0.002321,deriv 0.000918\n",
      "第19000次 0.101027,res 0.004197,deriv 0.096830\n",
      "第20000次的损失为0.262405\n",
      "Training time: 60.2795\n",
      "train_num:19, error:0.349387\n",
      "Start training! train_num:20\n",
      "第0次 2521.199951,res 52.592121,deriv 2468.607910\n",
      "第1000次 1.952416,res 1.303543,deriv 0.648873\n",
      "第2000次 0.108109,res 0.105439,deriv 0.002671\n",
      "第3000次 0.036237,res 0.032399,deriv 0.003837\n",
      "第4000次 0.012999,res 0.012263,deriv 0.000736\n",
      "第5000次 0.006365,res 0.005988,deriv 0.000377\n",
      "第6000次 1.579704,res 0.033347,deriv 1.546357\n",
      "第7000次 0.002481,res 0.002332,deriv 0.000149\n",
      "第8000次 0.002351,res 0.002211,deriv 0.000140\n",
      "第9000次 0.212228,res 0.007071,deriv 0.205157\n",
      "第10000次 0.002978,res 0.002799,deriv 0.000179\n",
      "第11000次 0.004561,res 0.004258,deriv 0.000303\n",
      "第12000次 0.006050,res 0.005525,deriv 0.000525\n",
      "第13000次 0.005380,res 0.005039,deriv 0.000341\n",
      "第14000次 0.017394,res 0.006538,deriv 0.010855\n",
      "第15000次 0.005072,res 0.004682,deriv 0.000390\n",
      "第16000次 0.006353,res 0.005920,deriv 0.000433\n",
      "第17000次 0.005397,res 0.004640,deriv 0.000758\n",
      "第18000次 0.006160,res 0.005709,deriv 0.000451\n",
      "第19000次 1.151922,res 0.025167,deriv 1.126756\n",
      "第20000次的损失为0.005427\n",
      "Training time: 66.1515\n",
      "train_num:20, error:0.346340\n"
     ]
    }
   ],
   "source": [
    "train_nums = np.arange(10, 21, 1)\n",
    "test_num   = 1000\n",
    "errors     = []\n",
    "for train_num in train_nums:\n",
    "    model = train_model(train_num)\n",
    "    error = compute_error(model, test_num)\n",
    "    print(\"train_num:%d, error:%f\"%(train_num, error))\n",
    "    errors.append(error)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "errors_gPINN = np.array(errors)\n",
    "np.save('errors_gPINN.npy', errors_gPINN)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# from tensorflow.python.client import device_lib as _device_lib\n",
    "# local_device_protos = _device_lib.list_local_devices()\n",
    "# devices = [x.name for x in local_device_protos]\n",
    "# for d in devices:\n",
    "# \tprint(d)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}