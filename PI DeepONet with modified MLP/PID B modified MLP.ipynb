{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "\n",
    "class PI_DeepONet:\n",
    "    def __init__(self, m, ics_data, bcs_data, res_data, branch_layer, trunk_layer, lr, activation, weight):\n",
    "        self.m = m\n",
    "        self.ics_data = ics_data\n",
    "        self.bcs_data = bcs_data\n",
    "        self.res_data = res_data\n",
    "        self.activation = activation\n",
    "        self.loss_log = []\n",
    "        self.loss_res_log = []\n",
    "        self.loss_ics_log = []\n",
    "        self.loss_bcs_log = []\n",
    "        self.weight = weight\n",
    "\n",
    "        self.branch_layer = branch_layer\n",
    "        self.trunk_layer  = trunk_layer\n",
    "\n",
    "        self.sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "        self.branch_weights, self.branch_biases, \\\n",
    "            self.b_W1, self.b_b1, self.b_W2, self.b_b2 = self.initilize_NN(branch_layer)\n",
    "        self.trunk_weights, self.trunk_biases, \\\n",
    "            self.t_W1, self.t_b1, self.t_W2, self.t_b2 = self.initilize_NN(trunk_layer)\n",
    "        self.bias = tf.Variable(tf.zeros([1], dtype=tf.float32), dtype=tf.float32)\n",
    "        # initial condition\n",
    "        self.u_i_tf  = tf.placeholder(tf.float32, shape=[None, self.m])\n",
    "        self.t_i_tf  = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        self.x_i_tf  = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        self.s_i_tf  = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        # boundary condition\n",
    "        self.u_b_tf  = tf.placeholder(tf.float32, shape=[None, self.m])\n",
    "        self.t1_b_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        self.x1_b_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        self.t2_b_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        self.x2_b_tf = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        self.s_b_tf  = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        # collocation points\n",
    "        self.u_r_tf  = tf.placeholder(tf.float32, shape=[None, self.m])\n",
    "        self.t_r_tf  = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        self.x_r_tf  = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "        self.s_r_tf  = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "\n",
    "        self.operator = self.operator_net(self.u_r_tf, self.t_r_tf, self.x_r_tf)\n",
    "        self.residual = self.residual_net(self.u_r_tf, self.t_r_tf, self.x_r_tf)\n",
    "        self.s_r      = self.loss_res(self.u_r_tf, self.t_r_tf, self.x_r_tf, self.s_r_tf)\n",
    "        self.s_i      = self.loss_ics(self.u_i_tf, self.t_i_tf, self.x_i_tf, self.s_i_tf)\n",
    "        self.s_b      = self.loss_bcs(self.u_b_tf, self.t1_b_tf, self.x1_b_tf,\n",
    "                                      self.t2_b_tf, self.x2_b_tf, self.s_b_tf)\n",
    "        self.loss     = self.weight*self.s_r + self.weight*self.s_i + self.s_b\n",
    "\n",
    "        steps_per_decay = 1000\n",
    "        decay_factor = 0.9\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.lr = tf.train.exponential_decay(learning_rate = lr,\n",
    "                                           global_step = self.global_step,\n",
    "                                           decay_steps = steps_per_decay,\n",
    "                                           decay_rate = decay_factor,\n",
    "                                           staircase = True\n",
    "                                           )\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss,\n",
    "                                            global_step=self.global_step)\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    def callback(self, loss, sr, si, sb):\n",
    "        print('Loss:%f,ic:%f,bc:%f,res:%f'%(loss, si, sb, sr))\n",
    "\n",
    "    def initilize_NN(self, layers):\n",
    "        weights = []\n",
    "        biases  = []\n",
    "        num_layers = len(layers)\n",
    "        for l in range(0, num_layers - 1):\n",
    "            W = self.xavier_init(size=[layers[l], layers[l+1]])\n",
    "            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "            weights.append(W)\n",
    "            biases.append(b)\n",
    "        # parameters of U\n",
    "        W1 = self.xavier_init([layers[0], layers[1]])\n",
    "        b1 = tf.Variable(tf.zeros([1,layers[1]], dtype=tf.float32), dtype=tf.float32)\n",
    "        # parameters of V\n",
    "        W2 = self.xavier_init([layers[0], layers[1]])\n",
    "        b2 = tf.Variable(tf.zeros([1,layers[1]], dtype=tf.float32), dtype=tf.float32)\n",
    "        return weights, biases, W1, b1, W2, b2\n",
    "\n",
    "    def xavier_init(self, size):\n",
    "        in_dim = size[0]\n",
    "        out_dim = size[1]\n",
    "        xavier_stddev = np.sqrt(2/(in_dim + out_dim))\n",
    "        return tf.Variable(tf.truncated_normal([in_dim, out_dim], stddev=xavier_stddev), dtype=tf.float32)\n",
    "\n",
    "    def neural_net(self, X, weights, biases, activation, W1, b1, W2, b2):\n",
    "        num_layers = len(weights) + 1\n",
    "        U = activation(tf.add(tf.matmul(X, W1), b1))\n",
    "        V = activation(tf.add(tf.matmul(X, W2), b2))\n",
    "        H = X\n",
    "        for l in range(0, num_layers - 2):\n",
    "            W = weights[l]\n",
    "            b = biases[l]\n",
    "            H = activation(tf.add(tf.matmul(H, W), b))\n",
    "            H = tf.multiply(1 - H, U) + tf.multiply(H, V)\n",
    "        W = weights[-1]\n",
    "        b = biases[-1]\n",
    "        Y = tf.add(tf.matmul(H, W), b)\n",
    "        return Y\n",
    "\n",
    "    def train(self, max_iter=40000, batch_size=50):\n",
    "        loss_value = np.inf\n",
    "        saver=tf.train.Saver(max_to_keep=1)\n",
    "        for iter in range(max_iter):\n",
    "            self.sess.run(self.global_step)\n",
    "            u_i, y_i, s_i = self.ics_data.get_batch()\n",
    "            u_b, y_b, s_b = self.bcs_data.get_batch()\n",
    "            u_r, y_r, s_r = self.res_data.get_batch()\n",
    "            tf_dict = {\n",
    "                self.u_r_tf  : u_r,\n",
    "                self.t_r_tf  : y_r[:, 0][:, None],\n",
    "                self.x_r_tf  : y_r[:, 1][:, None],\n",
    "                self.s_r_tf  : s_r,\n",
    "                self.u_i_tf  : u_i,\n",
    "                self.t_i_tf  : y_i[:, 0][:, None],\n",
    "                self.x_i_tf  : y_i[:, 1][:, None],\n",
    "                self.s_i_tf  : s_i,\n",
    "                self.u_b_tf  : u_b,\n",
    "                self.t1_b_tf : y_b[:, 0][:, None],\n",
    "                self.x1_b_tf : y_b[:, 1][:, None],\n",
    "                self.t2_b_tf : y_b[:, 2][:, None],\n",
    "                self.x2_b_tf : y_b[:, 3][:, None],\n",
    "                self.s_b_tf  : s_b\n",
    "            }\n",
    "            _, loss_value, si, sb, sr= self.sess.run([self.optimizer,\n",
    "                self.loss, self.s_i, self.s_b, self.s_r], tf_dict)\n",
    "            self.loss_res_log.append(sr)\n",
    "            self.loss_bcs_log.append(sb)\n",
    "            self.loss_ics_log.append(si)\n",
    "            self.loss_log.append(loss_value)\n",
    "            self.sess.run(self.lr)\n",
    "            saver.save(self.sess, \"./train/PID B modified MLP/ckpt/model.ckpt\", global_step=iter+1)\n",
    "            if iter % 100 == 0:\n",
    "                print(\"第%d次 %f,ic %f,bc %f,res %f\"%(iter, loss_value, si, sb, sr))\n",
    "        # model_file=tf.train.latest_checkpoint('./train/PID B Un weight/ckpt/')\n",
    "        # saver.restore(self.sess, model_file)\n",
    "        print(\"第%d次的损失为%f\"%(max_iter, loss_value))\n",
    "        print(\"最终lr为%f\"%(self.sess.run(self.lr)))\n",
    "\n",
    "    def operator_net(self, u, t, x):\n",
    "        branch_out = self.neural_net(u, self.branch_weights, self.branch_biases, self.activation,\n",
    "                                     self.b_W1, self.b_b1, self.b_W2, self.b_b2)\n",
    "        trunk_out  = self.neural_net(tf.concat([t, x], 1), self.trunk_weights, self.trunk_biases, self.activation,\n",
    "                                     self.t_W1, self.t_b1, self.t_W2, self.t_b2)\n",
    "        s = tf.reshape(tf.reduce_sum(tf.multiply(branch_out, trunk_out),\n",
    "                     axis=1) + self.bias, [-1, 1])\n",
    "        return s\n",
    "\n",
    "    def residual_net(self, u, t, x):\n",
    "        s_pred = self.operator_net(u, t, x)\n",
    "        s_t  = tf.gradients(s_pred, t)[0]\n",
    "        s_x  = tf.gradients(s_pred, x)[0]\n",
    "        s_xx = tf.gradients(s_x, x)[0]\n",
    "        f    = s_t + s_pred * s_x - 0.01 * s_xx\n",
    "        return f\n",
    "\n",
    "    def predict_s(self, u, t, x):\n",
    "        s = self.sess.run(self.operator, {self.u_r_tf: u, self.t_r_tf : t, self.x_r_tf: x})\n",
    "        return s\n",
    "\n",
    "    def predict_f(self, u, t, x):\n",
    "        s_r = self.sess.run(self.residual, {self.u_r_tf: u, self.t_r_tf : t, self.x_r_tf: x})\n",
    "        return s_r\n",
    "\n",
    "    def loss_res(self, u, t, x, s):\n",
    "        f = self.residual_net(u, t, x)\n",
    "        loss = tf.reduce_mean(tf.square(f - s))\n",
    "        return loss\n",
    "\n",
    "    def loss_ics(self, u, t, x, s):\n",
    "        s_pred = self.operator_net(u, t, x)\n",
    "        loss   =  tf.reduce_mean(tf.square(s_pred - s))\n",
    "        return loss\n",
    "\n",
    "    def loss_bcs(self, u, t1, x1, t2, x2, s):\n",
    "        s_pred_1 = self.operator_net(u, t1, x1)\n",
    "        s_pred_2 = self.operator_net(u, t2, x2)\n",
    "        s_x_1    = tf.gradients(s_pred_1, x1)[0]\n",
    "        s_x_2    = tf.gradients(s_pred_2, x2)[0]\n",
    "        loss_bc1 = tf.reduce_mean(tf.square(s_pred_1 - s_pred_2))\n",
    "        loss_bc2 = tf.reduce_mean(tf.square(s_x_1 - s_x_2))\n",
    "        return loss_bc1 + loss_bc2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    def __init__(self, u, y, s, batch_size):\n",
    "        self.u = u\n",
    "        self.y = y\n",
    "        self.s = s\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_batch(self):\n",
    "        N = self.u.shape[0]\n",
    "        index = np.random.randint(0, N, self.batch_size)\n",
    "        u_ = self.u[index, :]\n",
    "        y_ = self.y[index, :]\n",
    "        s_ = self.s[index, :]\n",
    "        return u_, y_, s_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Geneate ics training data\n",
    "def generate_one_ics_data(u0, m=101, P=101):\n",
    "    t_0 = np.zeros((P, 1))\n",
    "    x_0 = np.linspace(0, 1, P)[:, None]\n",
    "    y = np.hstack([t_0, x_0])         # y_shape=(P, 2)\n",
    "    u = np.tile(u0[None, :], (P, 1))  # u_shape=(P, m)\n",
    "    s = u0                            # s_shape=(m,  )\n",
    "\n",
    "    return u, y, s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Geneate bcs training data\n",
    "def generate_one_bcs_data(u0, m=101, P=101):\n",
    "    t_bc = np.random.rand(P)[:, None]\n",
    "    x_bc1 = np.zeros((P, 1))\n",
    "    x_bc2 = np.ones((P, 1))\n",
    "    y1 = np.hstack([t_bc, x_bc1])  # shape = (P, 2)\n",
    "    y2 = np.hstack([t_bc, x_bc2])  # shape = (P, 2)\n",
    "\n",
    "    u = np.tile(u0, (P, 1))        # shape = (P, m)\n",
    "    y = np.hstack([y1, y2])        # shape = (P, 4)\n",
    "    s = np.zeros((P, 1))           # shape = (P, 1)\n",
    "\n",
    "    return u, y, s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Geneate res training data\n",
    "def generate_one_res_data(u0, m=101, P=1000):\n",
    "    t_res = np.random.rand(P)[:, None]\n",
    "    x_res = np.random.rand(P)[:, None]\n",
    "\n",
    "    u = np.tile(u0, (P, 1))         # shape=(P, m)\n",
    "    y = np.hstack([t_res, x_res])   # shape=(P, 2)\n",
    "    s = np.zeros((P, 1))            # shape=(P, 1)\n",
    "\n",
    "    return u, y, s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "def generate_one_test_data(idx, usol, m=101, P=101):\n",
    "    u = usol[idx]\n",
    "    u0 = u[0,:]\n",
    "\n",
    "    t = np.linspace(0, 1, P)\n",
    "    x = np.linspace(0, 1, P)\n",
    "    T, X = np.meshgrid(t, x)\n",
    "\n",
    "    s = u.T.flatten()            # shape=(P**2, 1)\n",
    "    u = np.tile(u0, (P**2, 1))   # shape=(P**2, m)\n",
    "    y = np.hstack([T.flatten()[:,None], X.flatten()[:,None]]) # shape=(P**2, 2)\n",
    "\n",
    "    return u, y, s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def compute_error(idx, usol, m, P, model):\n",
    "    # 生成一组测试数据，计算相对L2误差，使用jax.vmap较方便\n",
    "    u_test, y_test, s_test = generate_one_test_data(idx, usol, m, P)\n",
    "\n",
    "    u_test = u_test.reshape(P**2,-1)\n",
    "    y_test = y_test.reshape(P**2,-1)\n",
    "    s_test = s_test.reshape(P**2,-1)\n",
    "    s_pred = model.predict_s(u_test, y_test[:, 0][:, None], y_test[:, 1][:, None])\n",
    "    error = np.linalg.norm(s_test - s_pred) / np.linalg.norm(s_test)\n",
    "\n",
    "    return error\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "path = './Burgers/Burger.mat'\n",
    "data = scipy.io.loadmat(path)\n",
    "usol = np.array(data['output'])\n",
    "N = usol.shape[0]  # number of total input samples\n",
    "N_train =1000      # number of input samples used for training\n",
    "N_test = N - N_train  # number of input samples used for test\n",
    "m = 101            # number of sensors for input samples\n",
    "P_ics_train = 101   # number of locations for evulating the initial condition\n",
    "P_bcs_train = 100    # number of locations for evulating the boundary condition\n",
    "P_res_train = 2500   # number of locations for evulating the PDE residual\n",
    "P_test = 101        # resolution of uniform grid for the test data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# 生成数据，并将数据处理成PI_DeepONet的输入类型\n",
    "u0_train = usol[:N_train,0,:]   # input samples\n",
    "u_ics = []\n",
    "y_ics = []\n",
    "s_ics = []\n",
    "for i in range(u0_train.shape[0]):\n",
    "    u, y, s = generate_one_ics_data(u0_train[i], m, P_ics_train)\n",
    "    u_ics.append(u)\n",
    "    y_ics.append(y)\n",
    "    s_ics.append(s)\n",
    "\n",
    "u_bcs = []\n",
    "y_bcs = []\n",
    "s_bcs = []\n",
    "for i in range(u0_train.shape[0]):\n",
    "    u, y, s = generate_one_bcs_data(u0_train[i], m, P_bcs_train)\n",
    "    u_bcs.append(u)\n",
    "    y_bcs.append(y)\n",
    "    s_bcs.append(s)\n",
    "\n",
    "u_res = []\n",
    "y_res = []\n",
    "s_res = []\n",
    "for i in range(u0_train.shape[0]):\n",
    "    u, y, s = generate_one_res_data(u0_train[i], m, P_res_train)\n",
    "    u_res.append(u)\n",
    "    y_res.append(y)\n",
    "    s_res.append(s)\n",
    "u_res = np.reshape(np.array(u_res), (-1, m))\n",
    "u_ics = np.reshape(np.array(u_ics), (-1, m))\n",
    "u_bcs = np.reshape(np.array(u_bcs), (-1, m))\n",
    "y_res = np.reshape(np.array(y_res), (-1, 2))\n",
    "y_ics = np.reshape(np.array(y_ics), (-1, 2))\n",
    "y_bcs = np.reshape(np.array(y_bcs), (-1, 4))\n",
    "s_res = np.reshape(np.array(s_res), (-1, 1))\n",
    "s_ics = np.reshape(np.array(s_ics), (-1, 1))\n",
    "s_bcs = np.reshape(np.array(s_bcs), (-1, 1))\n",
    "\n",
    "batch_size = 5000\n",
    "ics_data = DataGenerator(np.array(u_ics), np.array(y_ics), np.array(s_ics), batch_size)\n",
    "bcs_data = DataGenerator(np.array(u_bcs), np.array(y_bcs), np.array(s_bcs), batch_size)\n",
    "res_data = DataGenerator(np.array(u_res), np.array(y_res), np.array(s_res), batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次 2.887422,ic 0.062486,bc 0.035194,res 0.080125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-28-4b66ba764e28>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mpi_deeponet_tanh\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mPI_DeepONet\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mm\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mics_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbcs_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mres_data\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbranch_layers\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrunk_layers\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtanh\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mstart_time\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m \u001B[0mpi_deeponet_tanh\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmax_iter\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m200000\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m \u001B[0melapsed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Training time: %.4f'\u001B[0m \u001B[1;33m%\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0melapsed\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-19-3a708ed768a7>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self, max_iter, batch_size)\u001B[0m\n\u001B[0;32m    133\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mloss_log\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mloss_value\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    134\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msess\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 135\u001B[1;33m             \u001B[0msaver\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msess\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"./train/PID B modified MLP/ckpt/model.ckpt\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mglobal_step\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0miter\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    136\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0miter\u001B[0m \u001B[1;33m%\u001B[0m \u001B[1;36m100\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    137\u001B[0m                 \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"第%d次 %f,ic %f,bc %f,res %f\"\u001B[0m\u001B[1;33m%\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miter\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_value\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\programfile\\python3.7.4\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001B[0m in \u001B[0;36msave\u001B[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs, save_debug_info)\u001B[0m\n\u001B[0;32m   1198\u001B[0m               \u001B[0mmeta_graph_filename\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1199\u001B[0m               \u001B[0mstrip_default_attrs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mstrip_default_attrs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1200\u001B[1;33m               save_debug_info=save_debug_info)\n\u001B[0m\u001B[0;32m   1201\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1202\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_is_empty\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\programfile\\python3.7.4\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001B[0m in \u001B[0;36mexport_meta_graph\u001B[1;34m(self, filename, collection_list, as_text, export_scope, clear_devices, clear_extraneous_savers, strip_default_attrs, save_debug_info)\u001B[0m\n\u001B[0;32m   1241\u001B[0m     return export_meta_graph(\n\u001B[0;32m   1242\u001B[0m         \u001B[0mfilename\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1243\u001B[1;33m         \u001B[0mgraph_def\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_default_graph\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mas_graph_def\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0madd_shapes\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1244\u001B[0m         \u001B[0msaver_def\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msaver_def\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1245\u001B[0m         \u001B[0mcollection_list\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcollection_list\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\programfile\\python3.7.4\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36mas_graph_def\u001B[1;34m(self, from_version, add_shapes)\u001B[0m\n\u001B[0;32m   3463\u001B[0m     \"\"\"\n\u001B[0;32m   3464\u001B[0m     \u001B[1;31m# pylint: enable=line-too-long\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3465\u001B[1;33m     \u001B[0mresult\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_as_graph_def\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfrom_version\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0madd_shapes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3466\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3467\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\programfile\\python3.7.4\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001B[0m in \u001B[0;36m_as_graph_def\u001B[1;34m(self, from_version, add_shapes)\u001B[0m\n\u001B[0;32m   3400\u001B[0m           \u001B[0mop\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_nodes_by_name\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mnode\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3401\u001B[0m           \u001B[1;32mif\u001B[0m \u001B[0mop\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3402\u001B[1;33m             node.attr[\"_output_shapes\"].list.shape.extend(\n\u001B[0m\u001B[0;32m   3403\u001B[0m                 [output.get_shape().as_proto() for output in op.outputs])\n\u001B[0;32m   3404\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mfunction_def\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mgraph\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlibrary\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfunction\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "branch_layers = [m, 100, 100, 100, 100, 100, 100, 100]\n",
    "trunk_layers =  [2, 100, 100, 100, 100, 100, 100, 100]\n",
    "weight = 20\n",
    "pi_deeponet_tanh = PI_DeepONet(m, ics_data, bcs_data, res_data, branch_layers, trunk_layers, lr, tf.tanh, weight)\n",
    "start_time = time.time()\n",
    "pi_deeponet_tanh.train(max_iter=200000)\n",
    "elapsed = time.time() - start_time\n",
    "print('Training time: %.4f' % (elapsed))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "error = []\n",
    "for idx in range(N_train, N):\n",
    "    error.append(compute_error(idx, usol, m, P_test, pi_deeponet_tanh))\n",
    "error = np.array(error)\n",
    "print(error.mean())\n",
    "# 0.10320831983015072"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "u = usol[1400]\n",
    "u0 = u[0,:]\n",
    "\n",
    "t = np.linspace(0, 1, P_test)\n",
    "x = np.linspace(0, 1, P_test)\n",
    "T, X = np.meshgrid(t, x)\n",
    "\n",
    "s = u.T.flatten()\n",
    "u = np.tile(u0, (P_test**2, 1))\n",
    "y = np.hstack([T.flatten()[:,None], X.flatten()[:,None]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot for loss function\n",
    "plt.figure(figsize = (6,5))\n",
    "# plt.plot(model.loss_log, lw=2)\n",
    "plt.plot(pi_deeponet_tanh.loss_ics_log, lw=2, label='ics')\n",
    "plt.plot(pi_deeponet_tanh.loss_bcs_log, lw=2, label='bcs')\n",
    "plt.plot(pi_deeponet_tanh.loss_res_log, lw=2, label='res')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss.pdf')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Plot for one generated data\n",
    "k = 1833 # index\n",
    "u = usol[k,:, :]\n",
    "u0 = usol[k,0,:]\n",
    "\n",
    "P_test = 101\n",
    "\n",
    "t = np.linspace(0, 1, P_test)\n",
    "x = np.linspace(0, 1, P_test)\n",
    "T, X = np.meshgrid(t, x)\n",
    "\n",
    "u_test = np.tile(u0, (P_test**2, 1))\n",
    "y_test = np.hstack([T.flatten()[:,None], X.flatten()[:,None]])\n",
    "s_test = u.flatten()[:,None]\n",
    "\n",
    "s_pred = pi_deeponet_tanh.predict_s(u_test, y_test[:, 0][:, None], y_test[:, 1][:, None])\n",
    "S_pred = griddata(y_test, s_pred.flatten(), (T, X), method='cubic')\n",
    "\n",
    "error_s = np.linalg.norm(u - S_pred.T, 2) / np.linalg.norm(u, 2)\n",
    "\n",
    "print(\"error_s: {:.3e}\".format(error_s))\n",
    "\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "plt.pcolor(T, X, u, cmap='jet')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$t$')\n",
    "plt.title('Exact $s(x,t)$')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.savefig('Exact s.pdf')\n",
    "\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "plt.pcolor(T, X, S_pred.T, cmap='jet')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$t$')\n",
    "plt.title('Predict $s(x,t)$')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.savefig('Pred s.pdf')\n",
    "\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "plt.pcolor(T, X, np.abs(S_pred.T - u), cmap='jet')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$t$')\n",
    "plt.title('Absolute error')\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.savefig('Absolute error.pdf')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}